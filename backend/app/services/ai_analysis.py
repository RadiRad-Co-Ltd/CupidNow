import re
import json
import logging
import os

logger = logging.getLogger(__name__)

# Lazy imports to reduce baseline memory
# jieba, snownlp, groq, google-genai are imported on first use
from app.services.parser import Message
from app.services.text_analysis import STOP_WORDS

_groq_client = None
_gemini_client = None

_NOISE_RE = re.compile(r"^[\d\W\s]+$|^(.)\1+$")


def _is_meaningful(content: str) -> bool:
    """Use jieba + STOP_WORDS to check if a message has real content.

    Segment the message, strip stop words / punctuation / numbers.
    If nothing remains â†’ trivial message, not worth sending to AI.
    """
    text = content.strip()
    if len(text) <= 1:
        return False
    if _NOISE_RE.match(text):
        return False

    import jieba
    words = jieba.lcut(text)
    substantive = [
        w for w in words
        if len(w) >= 2
        and w not in STOP_WORDS
        and not re.match(r"^[\d\W]+$", w)
        and not re.match(r"^(.)\1+$", w)
    ]
    return len(substantive) >= 1


def _sentiment_intensity(content: str) -> float:
    """Return 0~0.5: how emotionally charged this message is.

    SnowNLP returns 0 (negative) ~ 1 (positive).
    Intensity = distance from neutral (0.5).
    Strong positive (0.95) â†’ 0.45, strong negative (0.05) â†’ 0.45
    Neutral (0.50) â†’ 0.0
    """
    try:
        from snownlp import SnowNLP
        score = SnowNLP(content).sentiments
        return abs(score - 0.5)
    except Exception:
        return 0.0


def _get_groq_client():
    global _groq_client
    if _groq_client is None:
        from groq import AsyncGroq
        _groq_client = AsyncGroq()
    return _groq_client


def _get_gemini_client():
    global _gemini_client
    if _gemini_client is None:
        from google import genai
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            return None
        _gemini_client = genai.Client(api_key=api_key)
    return _gemini_client


def sample_messages(
    messages: list[Message], max_total: int = 500
) -> list[Message]:
    """Filter out trivial messages, then prioritize by sentiment intensity."""
    if not messages:
        return []

    # Phase 1: jieba + STOP_WORDS â€” remove noise
    meaningful = [
        m for m in messages
        if m.msg_type == "text" and _is_meaningful(m.content)
    ]

    # Phase 2: if within budget, return all
    if len(meaningful) <= max_total:
        return meaningful

    # Phase 3: uniform sampling to reduce to ~2x budget, then score
    # This avoids running SnowNLP on thousands of messages
    import random
    if len(meaningful) > max_total * 3:
        random.seed(42)
        meaningful = random.sample(meaningful, max_total * 3)

    scored = [(m, _sentiment_intensity(m.content)) for m in meaningful]
    scored.sort(key=lambda x: x[1], reverse=True)
    selected = [m for m, _ in scored[:max_total]]

    # Re-sort by timestamp so the AI sees messages in chronological order
    selected.sort(key=lambda m: m.timestamp)
    return selected


def _format_stats_block(stats: dict | None) -> str:
    """Format quantitative stats into a concise block for the AI prompt."""
    if not stats:
        return ""

    lines = ["â”€â”€ é‡åŒ–æ•¸æ“šï¼ˆä¾›è©•åˆ†åƒè€ƒï¼‰â”€â”€"]

    if "basicStats" in stats:
        bs = stats["basicStats"]
        mc = bs.get("messageCount", {})
        lines.append(f"ç¸½è¨Šæ¯æ•¸ï¼š{mc.get('total', 0):,}")
        persons = [k for k in mc if k != "total"]
        for p in persons:
            lines.append(f"  {p}ï¼š{mc.get(p, 0):,} å‰‡")
        dr = bs.get("dateRange", {})
        lines.append(f"èŠå¤©å¤©æ•¸ï¼š{dr.get('totalDays', 0)} å¤©ï¼ˆ{dr.get('start', '')} ~ {dr.get('end', '')}ï¼‰")
        cs = bs.get("callStats", {})
        if cs.get("totalCalls", 0) > 0:
            avg_min = round(cs.get("avgDurationSeconds", 0) / 60)
            lines.append(f"é€šè©±ï¼š{cs['completedCalls']} é€šï¼Œå¹³å‡ {avg_min} åˆ†é˜")

    if "replyBehavior" in stats:
        rb = stats["replyBehavior"]
        irr = rb.get("instantReplyRate", {})
        for p, rate in irr.items():
            lines.append(f"{p} ç§’å›ç‡ï¼š{round(rate * 100)}%")
        art = rb.get("avgReplyTime", {})
        for p, sec in art.items():
            lines.append(f"{p} å¹³å‡å›è¦†æ™‚é–“ï¼š{round(sec / 60, 1)} åˆ†é˜")
        lor = rb.get("leftOnRead", {})
        for p, cnt in lor.items():
            lines.append(f"{p} å·²è®€ä¸å›æ¬¡æ•¸ï¼š{cnt}")

    if "coldWars" in stats:
        cw = stats["coldWars"]
        if cw:
            lines.append(f"å†·æˆ°/ä½æ½®æœŸï¼š{len(cw)} æ¬¡")
        else:
            lines.append("å†·æˆ°/ä½æ½®æœŸï¼š0 æ¬¡")

    if "textAnalysis" in stats:
        ta = stats["textAnalysis"]
        wc = ta.get("wordCloud", {})
        if wc:
            lines.append("")
            lines.append("â”€â”€ é›™æ–¹é«˜é »è©ï¼ˆå·²å»é™¤åœç”¨è©ï¼Œå«å‡ºç¾æ¬¡æ•¸ï¼‰â”€â”€")
            for person, words in wc.items():
                top = words[:30]
                if top:
                    items = ", ".join(f"{w['word']}({w['count']})" for w in top)
                    lines.append(f"{person}ï¼š{items}")

    return chr(10).join(lines)


def build_prompt(messages: list[Message], persons: list[str], stats: dict | None = None) -> str:
    p1 = persons[0]
    p2 = persons[1] if len(persons) > 1 else "Person2"

    # Split messages by person for context
    p1_lines, p2_lines = [], []
    for m in messages:
        content = m.content[:80] if len(m.content) > 80 else m.content
        line = f"[{m.timestamp.strftime('%m/%d %H:%M')}] {content}"
        if m.sender == p1:
            p1_lines.append(line)
        else:
            p2_lines.append(line)

    # Also build interleaved timeline for context
    timeline = []
    for m in messages:
        content = m.content[:80] if len(m.content) > 80 else m.content
        timeline.append(f"[{m.timestamp.strftime('%m/%d %H:%M')}] {m.sender}: {content}")

    stats_block = _format_stats_block(stats)

    return f"""ä½ æ˜¯ä¸€ä½è¶…ç´šæ‡‚æ„Ÿæƒ…çš„é–¨èœœåˆ†æå¸«ï¼Œèªªè©±æ´»æ½‘ã€å¸¶é»ä¿çš®ï¼Œæ“…é•·å¾èŠå¤©è¨˜éŒ„ä¸­çœ‹å‡ºå…©å€‹äººä¹‹é–“çš„å¾®å¦™äº’å‹•å’ŒåŒ–å­¸åæ‡‰ã€‚

ä»¥ä¸‹æ˜¯ {p1} å’Œ {p2} çš„èŠå¤©è¨˜éŒ„ã€‚

âš ï¸ é—œä¿‚åˆ¤æ–·è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼Œè«‹ä»”ç´°åˆ†æå¾Œå†ä¸‹çµè«–ï¼‰ï¼š
å…©äººå¯èƒ½æ˜¯ä»»ä½•é—œä¿‚â€”â€”åŒäº‹ã€æœ‹å‹ã€ç¶²å‹ã€æ›–æ˜§å°è±¡ã€æƒ…ä¾¶ã€è€å¤«è€å¦»ã€‚ä¸è¦å› ç‚ºå°è©±ä¸­æ—¥å¸¸ç‘£äº‹å¤šå°±ç›´æ¥å‡è¨­æ˜¯ã€Œè€å¤«è€å¦»ã€æˆ–ã€Œç©©å®šäº¤å¾€ã€ã€‚è«‹æ ¹æ“šä»¥ä¸‹ç·šç´¢ç¶œåˆåˆ¤æ–·ï¼š
- ç¨±å‘¼æ–¹å¼ï¼šç”¨ã€Œå¯¶è²/è¦ªæ„›çš„/è€å…¬è€å©†ã€vsã€Œå­¸é•·/åŒäº‹åã€vsã€Œä½ ã€å·®ç•°å¾ˆå¤§
- è©±é¡Œé‚Šç•Œï¼šæƒ…ä¾¶æœƒèŠç§å¯†å¿ƒäº‹ã€æ’’å¬Œåƒé†‹ï¼›åŒäº‹æœ‹å‹ä¸»è¦èŠå·¥ä½œã€å…±åŒè©±é¡Œ
- è‚¢é«”èªè¨€æš—ç¤ºï¼šæœ‰æ²’æœ‰ã€ŒæŠ±æŠ±/è¦ªè¦ª/æƒ³ä½ ã€ç­‰è¦ªå¯†è¡¨é”
- äº’å‹•é »ç‡èˆ‡æ™‚æ®µï¼šæ·±å¤œèŠå¤©ã€æ¯å¤©æ—©æ™šå•å€™ vs åªåœ¨ä¸Šç­æ™‚é–“èŠ
- æƒ…æ„Ÿæ¿ƒåº¦ï¼šæœ‰æ²’æœ‰æ˜é¡¯çš„æ›–æ˜§ã€åƒé†‹ã€æƒ³å¿µã€å¿ƒç–¼ç­‰æƒ…ç·’

åˆ¤æ–·å‡ºé—œä¿‚éšæ®µå¾Œï¼Œè©•åˆ†å’Œå»ºè­°éƒ½è¦ç¬¦åˆè©²éšæ®µçš„åˆç†æœŸå¾…ã€‚ä¾‹å¦‚ï¼š
- åŒäº‹/æœ‹å‹ï¼šä¸éœ€è¦æœ‰ç”œèœœè¨Šæ¯ï¼Œé‡é»çœ‹äº’å‹•å“è³ªå’Œé»˜å¥‘
- æ›–æ˜§ä¸­ï¼šçœ‹è©¦æ¢ã€æ”¾é›»ã€ä¸»å‹•ç¨‹åº¦
- ç©©å®šäº¤å¾€ï¼šçœ‹æ—¥å¸¸é—œå¿ƒã€è¡çªè™•ç†ã€æƒ…æ„Ÿç¶­ç¹«
- è€å¤«è€å¦»ï¼šæ—¥å¸¸ç‘£äº‹å¤šä½†ä»æœ‰é—œå¿ƒæ˜¯æ­£å¸¸çš„ï¼Œä¸æ‰£åˆ†

æ³¨æ„ï¼šè©•åˆ†æ™‚è«‹åŒæ™‚åƒè€ƒä¸‹æ–¹çš„é‡åŒ–æ•¸æ“šï¼Œä¾‹å¦‚ç§’å›ç‡é«˜ä»£è¡¨ä¸»å‹•æ€§å¼·ã€å·²è®€ä¸å›å¤šä»£è¡¨å¯èƒ½æœ‰å†·æ·¡å‚¾å‘ã€é€šè©±é »ç¹ä»£è¡¨æ„Ÿæƒ…è¼ƒè¦ªå¯†ã€‚

{stats_block}

â”€â”€ {p1} èªªçš„è©± â”€â”€
{chr(10).join(p1_lines[-80:])}

â”€â”€ {p2} èªªçš„è©± â”€â”€
{chr(10).join(p2_lines[-80:])}

â”€â”€ å®Œæ•´å°è©±æ™‚é–“è»¸ï¼ˆçœ‹äº’å‹•ç¯€å¥ï¼‰â”€â”€
{chr(10).join(timeline[-120:])}

è«‹ç¶œåˆä»¥ä¸Šå…§å®¹ï¼Œå›å‚³ä»¥ä¸‹ JSONï¼ˆä¸è¦åŠ  markdown code blockã€ä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "loveScore": {{
    "score": <0-100 å¿ƒå‹•æŒ‡æ•¸ï¼Œè«‹åƒè€ƒä»¥ä¸‹äº”å€‹ç¶­åº¦ç¶œåˆè©•åˆ†ï¼Œä½†ä½ ä¹Ÿå¯ä»¥æ ¹æ“šå°è©±ç‰¹è‰²è‡ªè¡Œèª¿æ•´æ¬Šé‡ï¼š
      ç”œåº¦/é—œå¿ƒé »ç‡ï¼ˆç´„ 25%ï¼‰ï¼šç”œèœœè¨Šæ¯çš„ä½”æ¯”å’Œæ¿ƒåº¦ï¼Œé•·æœŸä¼´ä¾¶çš„æ—¥å¸¸é—œå¿ƒï¼ˆåƒäº†å—ã€è·¯ä¸Šå°å¿ƒï¼‰ä¹Ÿç®—
      ä¸»å‹•æ€§å¹³è¡¡ï¼ˆç´„ 20%ï¼‰ï¼šé›™æ–¹ä¸»å‹•ç™¼è¨Šçš„æ¯”ä¾‹æ˜¯å¦å‡è¡¡ï¼Œé‚„æ˜¯å–®æ–¹é¢åœ¨è¿½
      æƒ…æ„Ÿè¡¨é”ï¼ˆç´„ 20%ï¼‰ï¼šæ›–æ˜§æœŸçœ‹èª¿æƒ…æ”¾é›»ï¼Œç©©å®šæœŸçœ‹æœ‰æ²’æœ‰æŒçºŒè¡¨é”æ„›æ„å’Œåœ¨ä¹
      é»˜å¥‘åº¦ï¼ˆç´„ 20%ï¼‰ï¼šå›è¦†é€Ÿåº¦ã€è©±é¡ŒéŠœæ¥é †æš¢åº¦ã€äº’ç›¸å‘¼æ‡‰çš„ç¨‹åº¦
      è¡çªä¿®å¾©åŠ›ï¼ˆç´„ 15%ï¼‰ï¼šåµæ¶æˆ–å†·æ·¡å¾Œå¤šå¿«å›æº«ã€æœ‰æ²’æœ‰ä¸»å‹•ç ´å†°
    >,
    "comment": "<80-120 å­—çš„æ´»æ½‘è©•èªï¼Œ2-3 å¥è©±ã€‚åƒé–¨èœœåœ¨æ—é‚Šå¹«ä½ åˆ†æï¼Œç¬¬ä¸€å¥é»å‡ºä½ å€‘çš„äº’å‹•ç‰¹è‰²æˆ–äº®é»ï¼Œç¬¬äºŒå¥å…·é«”æè¿°ä¸€å€‹è®“äººå°è±¡æ·±åˆ»çš„äº’å‹•æ¨¡å¼ï¼Œç¬¬ä¸‰å¥çµ¦å‡ºä¸€å¥æš–å¿ƒæˆ–ä¿çš®çš„ç¸½çµã€‚æ ¹æ“šé—œä¿‚éšæ®µçµ¦å‡ºä¸åŒé¢¨æ ¼çš„é»è©•ï¼ˆæ›–æ˜§æœŸå¯ä»¥ä¿çš®ï¼Œè€å¤«è€å¦»å¯ä»¥æº«é¦¨ï¼‰>"
  }},
  "sentiment": {{
    "sweet": <ç”œèœœæ’’å¬Œä½”æ¯” 0-100>,
    "flirty": <æ›–æ˜§æ”¾é›»ã€è©¦æ¢ã€èª¿æƒ…ä½”æ¯” 0-100>,
    "daily": <æŸ´ç±³æ²¹é¹½æ—¥å¸¸ä½”æ¯” 0-100>,
    "conflict": <ç«è—¥å‘³ã€å†·æ·¡ã€ä¸è€ç…©ä½”æ¯” 0-100>,
    "missing": <æƒ³å¿µã€æ¨ä¸å¾—ã€åœ¨æ„å°æ–¹ä½”æ¯” 0-100>
  }},
  "goldenQuotes": {{
    "sweetest": [
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}}
    ],
    "funniest": [
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}}
    ],
    "mostTouching": [
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}},
      {{"quote": "<åŸæ–‡>", "sender": "<èª°èªªçš„>", "date": "<å¹¾æœˆ/å¹¾æ—¥>"}}
    ]
  }},
  "relationshipType": "<ç”¨ä¸€å€‹è©æè¿°ä½ åˆ¤æ–·çš„é—œä¿‚é¡å‹ï¼šåŒäº‹ã€æœ‹å‹ã€ç¶²å‹ã€æ›–æ˜§ä¸­ã€ç†±æˆ€æœŸã€ç©©å®šäº¤å¾€ã€è€å¤«è€å¦»>",
  "insight": "<100 å­—ä»¥å…§ï¼Œç”¨æ´»æ½‘çš„èªæ°£æè¿° {p1} å’Œ {p2} çš„é—œä¿‚éšæ®µå’Œäº’å‹•æ¨¡å¼ã€‚å…ˆæ˜ç¢ºé»å‡ºä½ åˆ¤æ–·çš„é—œä¿‚é¡å‹å’Œä¾æ“šï¼Œå†æè¿°äº’å‹•ç‰¹è‰²ã€‚æœ‹å‹/åŒäº‹å°±åˆ†æé»˜å¥‘å’Œäº’å‹•å“è³ªï¼›æ›–æ˜§æœŸåˆ†æèª°åœ¨è¿½èª°ï¼›æƒ…ä¾¶åˆ†ææ„Ÿæƒ…æ¿ƒåº¦>",
  "sharedInterests": [
    {{
      "category": "<é¡åˆ¥åç¨±ï¼Œä¾‹å¦‚ï¼šæ„›å»çš„åœ°æ–¹ã€æ„›åƒçš„æ±è¥¿ã€æ„›çœ‹çš„åŠ‡ã€æ„›è½çš„éŸ³æ¨‚ã€å¸¸ä¸€èµ·åšçš„äº‹ã€å…±åŒçš„å—œå¥½ï¼Œæˆ–ä»»ä½•ä½ å¾å°è©±ä¸­ç™¼ç¾çš„å…±åŒèˆˆè¶£é¡åˆ¥>",
      "items": ["<å…·é«”åç¨±1>", "<å…·é«”åç¨±2>", "<å…·é«”åç¨±3>"]
    }}
  ],
  "advice": [
    {{"category": "ğŸ’¬ èŠå¤©æŠ€å·§", "target": "{p1}", "content": "<æ ¹æ“š {p1} çš„èŠå¤©é¢¨æ ¼ï¼Œçµ¦ä¸€å¥å…·é«”ã€å¯åŸ·è¡Œçš„æºé€šå»ºè­°ï¼Œä¾‹å¦‚å›è¦†é€Ÿåº¦ã€è¡¨é”æ–¹å¼ã€ä¸»å‹•ç¨‹åº¦ç­‰>"}},
    {{"category": "ğŸ’¬ èŠå¤©æŠ€å·§", "target": "{p2}", "content": "<æ ¹æ“š {p2} çš„èŠå¤©é¢¨æ ¼ï¼Œçµ¦ä¸€å¥å…·é«”ã€å¯åŸ·è¡Œçš„æºé€šå»ºè­°>"}},
    {{"category": "â¤ï¸ æ„Ÿæƒ…å¢æº«", "target": "å…©äºº", "content": "<ä¸€å€‹å…·é«”çš„äº’å‹•å»ºè­°ï¼Œä¾‹å¦‚å¯ä»¥å˜—è©¦çš„è©±é¡Œã€å°éŠæˆ²ã€æˆ–è®“å°è©±æ›´æœ‰æº«åº¦çš„æ–¹æ³•>"}},
    {{"category": "ğŸ¯ ç´„æœƒéˆæ„Ÿ", "target": "å…©äºº", "content": "<æ ¹æ“šå°è©±ä¸­æåˆ°çš„åœ°é»ã€é£Ÿç‰©ã€èˆˆè¶£ï¼Œæ¨è–¦ä¸€å€‹å…·é«”çš„ç´„æœƒæˆ–æ´»å‹•é»å­>"}},
    {{"category": "âš¡ é»˜å¥‘å‡ç´š", "target": "å…©äºº", "content": "<é‡å°ç›®å‰äº’å‹•æ¨¡å¼ä¸­å¯ä»¥æ”¹å–„çš„åœ°æ–¹ï¼Œä¾‹å¦‚å›è¦†ç¯€å¥ä¸åŒæ­¥ã€è©±é¡Œæ·±åº¦ä¸å¤ ã€æˆ–æŸæ–¹å¤ªè¢«å‹•ç­‰ï¼Œçµ¦å‡ºå…·é«”å»ºè­°>"}},
    {{"category": "ğŸŒŸ é—œä¿‚æˆé•·", "target": "å…©äºº", "content": "<æ ¹æ“šåˆ¤æ–·å‡ºçš„é—œä¿‚éšæ®µï¼Œçµ¦ä¸€å€‹å¹«åŠ©é—œä¿‚é€²éšçš„å»ºè­°ã€‚æ›–æ˜§æœŸï¼šæ€éº¼æ›´æ˜ç¢ºè¡¨é”å¿ƒæ„ï¼›ç©©å®šæœŸï¼šæ€éº¼ä¿æŒæ–°é®®æ„Ÿï¼›è€å¤«è€å¦»ï¼šæ€éº¼é‡æ–°æ‰¾å›å¿ƒå‹•>"}}
  ]
}}"""


class AIRateLimitError(Exception):
    """Raised when all AI providers are rate limited."""
    pass


def _parse_ai_response(text: str, provider: str) -> dict | None:
    """Parse AI response text into dict. Returns None on failure."""
    text = text.strip()

    # Try direct parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # Try extracting from markdown code block
    if "```" in text:
        try:
            json_str = text.split("```")[1]
            if json_str.startswith("json"):
                json_str = json_str[4:]
            return json.loads(json_str.strip())
        except (json.JSONDecodeError, IndexError):
            pass

    logger.error("[%s] JSON parse failed, text preview: %s", provider, text[:500])
    return None


async def _call_groq(prompt: str) -> dict | None:
    """Try Groq API. Returns parsed dict, None on parse failure, raises on rate limit."""
    client = _get_groq_client()

    try:
        response = await client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            max_tokens=3000,
            temperature=0.5,
            messages=[{"role": "user", "content": prompt}],
        )
    except Exception as e:
        if "429" in str(e) or "rate_limit" in str(e).lower():
            logger.warning("[Groq] Rate limited: %s", e)
            return None  # Signal to try fallback
        raise

    text = response.choices[0].message.content.strip()
    finish_reason = response.choices[0].finish_reason

    if finish_reason != "stop":
        logger.warning("[Groq] Response truncated (finish_reason=%s), length=%d", finish_reason, len(text))

    result = _parse_ai_response(text, "Groq")
    if result:
        logger.info("[Groq] AI analysis succeeded")
    return result


async def _call_gemini(prompt: str) -> dict | None:
    """Try Google Gemini API as fallback."""
    client = _get_gemini_client()
    if client is None:
        logger.warning("[Gemini] No GOOGLE_API_KEY configured, skipping")
        return None

    try:
        response = await client.aio.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
        )
    except Exception as e:
        if "429" in str(e) or "rate" in str(e).lower():
            logger.warning("[Gemini] Rate limited: %s", e)
            return None
        logger.exception("[Gemini] API call failed")
        return None

    text = response.text or ""

    result = _parse_ai_response(text, "Gemini")
    if result:
        logger.info("[Gemini] AI analysis succeeded (fallback)")
    return result


async def analyze_with_ai(
    messages: list[Message], persons: list[str], stats: dict | None = None,
) -> dict:
    """Call AI API with Groq â†’ Gemini fallback chain."""
    sampled = sample_messages(messages)
    if not sampled:
        return _fallback_result()

    prompt = build_prompt(sampled, persons, stats)

    # 1. Try Groq (faster)
    result = await _call_groq(prompt)
    if result:
        return result

    # 2. Fallback to Gemini
    logger.info("Groq unavailable, falling back to Gemini")
    result = await _call_gemini(prompt)
    if result:
        return result

    # 3. Both failed
    logger.error("All AI providers failed")
    raise AIRateLimitError("AI åˆ†ææœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦")


def _fallback_result() -> dict:
    return {
        "loveScore": {"score": 50, "comment": "è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•å®Œæ•´åˆ†æ"},
        "sentiment": {"sweet": 20, "flirty": 20, "daily": 40, "conflict": 10, "missing": 10},
        "goldenQuotes": {"sweetest": [], "funniest": [], "mostTouching": []},
        "insight": "å°è©±å…§å®¹ä¸è¶³ï¼Œå»ºè­°ä¸Šå‚³æ›´é•·çš„å°è©±è¨˜éŒ„ä»¥ç²å¾—æ›´æº–ç¢ºçš„åˆ†æã€‚",
        "advice": [],
    }
